{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limin/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to /home/limin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/limin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#文件目录\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './model_weights/glove.6B/'\n",
    "SAVE_DIR = './model_weights/'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')# 下载停止词，即不能表现内容意义的词，如：'ourselves', 'between', 'but', 'again', 'there'\n",
    "nltk.download('punkt')# 下载分词工具\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将预处理所有文章并将其转换为特征向量，以便将其输入到RNN中。\n",
    "\n",
    "这些都是用于整理文章数据的辅助函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"清洗句子/文章，得到句子/文章的词列表\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)  # 去除文章中非大小写字母以外的字符\n",
    "    words = essay_v.lower().split() #小写，分词成词列表\n",
    "    # 去除停止符\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"将文章分句，并调用essay_to_wordlist（）对句子处理\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')#加载英文的划分句子的模型(英文句子特点：.之后有空格)\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())#得到句子列表 #strip()去首尾的空格\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"从文章的单词列表中制作特征向量\"\"\"\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word) #训练集中出现的词列表\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec, model[word])#将每个词向量叠加\n",
    "    featureVec = np.divide(featureVec, num_words)#文章的特征向量为文章中词向量的平均\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"将文章集生成word2vec模型的词向量\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays), num_features), dtype=\"float32\")\n",
    "    # 每篇文章的特征向量\n",
    "    for essay in essays: # 对每个文章向量化调用makeFeatureVec()向量化\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们定义了一个2层LSTM模型。\n",
    "\n",
    "因为我们没有对训练标签进行标准化，我们将在输出层中使用Relu，而不是Sigmoid激活。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    \"\"\"构建RNN模型\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    # 对网络的学习过程进行配置，损失函数为均方误差，评价参数为平均绝对误差\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()# 输出模型各层的参数状况\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')  # 读取文件\n",
    "X = X.dropna(axis=1)#删除缺省的属性\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])#删除各评委的打分\n",
    "\n",
    "[r, c] = X.shape\n",
    "y = X['domain1_score']  # 文章分数y：两位评委对文章的评分和\n",
    "max_score = [12, 6, 3, 3, 4, 4, 30, 60]\n",
    "\n",
    "for i in range(r):\n",
    "\n",
    "    for j in range(8):\n",
    "        if X.iloc[i, 1] == j + 1:\n",
    "           \n",
    "            X.iloc[i, 3] =X.iloc[i, 3] /max_score[j]\n",
    "           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0       0.666667  \n",
       "1       0.750000  \n",
       "2       0.583333  \n",
       "3       0.833333  \n",
       "4       0.666667  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们在数据集上训练模型。\n",
    "\n",
    "我们将使用5折交叉验证，并测量每折的二次加权Kappa。 然后，我们将计算所有折的平均kappa值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)  # 5折交叉验证\n",
    "results = []\n",
    "y_pred_list = []\n",
    "count = 1\n",
    "\n",
    "\n",
    "for traincv, testcv in cv.split(X):# 将数据集划分成训练集和测试集，返回5组索引\n",
    "\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "\n",
    "    \"\"\"划分训练集和测试集\"\"\"\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "\n",
    "\n",
    "\n",
    "    '''word2vec模型'''\n",
    "    sentences = []\n",
    "\n",
    "    # 从训练集中获取所有句子及分词\n",
    "    for essay in train_essays:\n",
    "        sentences += essay_to_sentences(essay, remove_stopwords=True)\n",
    "\n",
    "    # word2vec模型的参数\n",
    "    num_features = 300  # 特征向量的维度\n",
    "    min_word_count = 40  # 最小词频，小于min_word_count的词被丢弃\n",
    "    num_workers = 4  # 训练的并行数\n",
    "    context = 10 # 当前词与预测词在一个句子中的最大距离\n",
    "    downsampling = 1e-3 # 高频词汇的随机降采样的配置阈值\n",
    "\n",
    "    # 训练模型\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context,\n",
    "                     sample=downsampling)\n",
    "    model.init_sims(replace=True)  # 结束训练后锁定模型，使模型的存储更加高效\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True) # 保存模型\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''LSTM模型'''\n",
    "\n",
    "    # 用word2vec模型向量化训练和测试数据中文章\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:  # 生成文章的词列表\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)  # 向量化的文章集\n",
    "\n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
    "\n",
    "    # 转换训练向量和测试向量为numpy数组，提高运行效率\n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # 将训练向量和测试向量重塑为3维 (1代表一个时间步长)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "\n",
    "    # 训练lstm模型\n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=40)\n",
    "    # lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "\n",
    "    # 使用测试集预测模型输出\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "\n",
    "    # 存储5个模型中最后一个.\n",
    "    if count == 5:\n",
    "        lstm_model.save('./model_weights/final_lstm.h5')\n",
    "\n",
    "    # 评估测试结果\n",
    "    y_pred = np.around(y_pred) # 将预测值y_pred舍入到最接近的整数\n",
    "    result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic') # 获取二次均值平均kappa值\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.9591\n"
     ]
    }
   ],
   "source": [
    "# 输出五次训练的平均kappa值\n",
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
